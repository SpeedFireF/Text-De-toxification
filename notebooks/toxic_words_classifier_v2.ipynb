{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ToxicWordClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, dropout_rate=0.2):\n",
    "        super(ToxicWordClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)  # Apply dropout to the embedded input\n",
    "        output = self.fc(embedded)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load toxic and non-toxic words from external text files\n",
    "toxic_words = [line.strip() for line in open('/Users/damirabdulaev/Downloads/toxic_words.txt', 'r', encoding='utf-8')]\n",
    "non_toxic_words = [line.strip() for line in open('/Users/damirabdulaev/Downloads/positive-words.txt', 'r', encoding='utf-8')]\n",
    "all_words = toxic_words + non_toxic_words\n",
    "labels = [1] * len(toxic_words) + [0] * len(non_toxic_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and convert your sentences to model input\n",
    "sentences = all_words  # Replace with your list of sentences\n",
    "\n",
    "# Tokenize and convert sentences to input indices\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence and add special tokens\n",
    "    encoded_dict = tokenizer(\n",
    "        sentence,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=1,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Extract the input IDs and attention mask\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "# Convert the lists of tensors to a single tensor\n",
    "word_indices = torch.cat(input_ids, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1018],\n",
      "        [ 1019],\n",
      "        [ 1019],\n",
      "        ...,\n",
      "        [28672],\n",
      "        [27838],\n",
      "        [14101]])\n"
     ]
    }
   ],
   "source": [
    "print(word_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/7240 [00:00<?, ?it/s]/var/folders/8d/4djwtz9j45b6chzl1l6yzrg40000gn/T/ipykernel_40617/3970738860.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(indices, dtype=torch.long)\n",
      "Epoch 1: 100%|██████████| 7240/7240 [01:41<00:00, 71.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8] - Loss: 0.6094, Accuracy: 72.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 7240/7240 [01:41<00:00, 71.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8] - Loss: 0.5424, Accuracy: 76.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7240/7240 [01:41<00:00, 71.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8] - Loss: 0.4896, Accuracy: 78.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 7240/7240 [01:40<00:00, 72.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8] - Loss: 0.4300, Accuracy: 81.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7240/7240 [01:41<00:00, 71.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8] - Loss: 0.3828, Accuracy: 83.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 7240/7240 [01:39<00:00, 72.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8] - Loss: 0.3378, Accuracy: 86.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 7240/7240 [01:39<00:00, 72.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8] - Loss: 0.2987, Accuracy: 88.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7240/7240 [01:40<00:00, 72.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8] - Loss: 0.2710, Accuracy: 89.49%\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a PyTorch model\n",
    "vocab_size = len(tokenizer.vocab)  # Assuming you've defined 'vocabulary'\n",
    "embedding_dim = 100  # Adjust as needed\n",
    "output_dim = 1  # Assuming binary classification\n",
    "\n",
    "model = ToxicWordClassifier(vocab_size, embedding_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert labels to tensors\n",
    "labels = torch.tensor(labels, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8  # Specify the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    # Wrap your training data with tqdm for the progress bar\n",
    "    for indices, label in tqdm(zip(word_indices, labels), total=len(labels), desc=f'Epoch {epoch + 1}'):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)[0][0]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct = (predicted == label).float()\n",
    "        total_correct += correct.sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = (total_correct / total_samples) * 100.0\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'twc.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                          reference   \n0           0  If Alkar is flooding her with psychic waste, t...  \\\n1           1                          Now you're getting nasty.   \n2           2           Well, we could spare your life, for one.   \n3           3          Ah! Monkey, you've got to snap out of it.   \n4           4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff   \n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309  \\\n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = '/Users/damirabdulaev/Downloads/filtered.tsv'\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/damirabdulaev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/577777 [00:00<11:40, 824.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "If Alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.\n",
      "Reconstructed Sentence:\n",
      "if al [MASK] is flooding her with psychic [MASK], [MASK] [MASK] the high level of [MASK] [MASK]transmitters.\n",
      "Original Sentence:\n",
      "Now you're getting nasty.\n",
      "Reconstructed Sentence:\n",
      "now you're [MASK] [MASK].\n",
      "Original Sentence:\n",
      "Well, we could spare your life, for one.\n",
      "Reconstructed Sentence:\n",
      "well, [MASK] could spare your life, [MASK] [MASK].\n",
      "Original Sentence:\n",
      "Ah! Monkey, you've got to snap out of it.\n",
      "Reconstructed Sentence:\n",
      "ah! [MASK], you've got [MASK] snap out of [MASK].\n",
      "Original Sentence:\n",
      "I've got orders to put her down.\n",
      "Reconstructed Sentence:\n",
      "i've got orders [MASK] put her [MASK].\n",
      "Original Sentence:\n",
      "I'm not gonna have a child... ...with the same genetic disorder as me who's gonna die. L...\n",
      "Reconstructed Sentence:\n",
      "i'[MASK] [MASK] gonna have a child...... with the same genetic [MASK] as me who'[MASK] gonna die. [MASK]...\n",
      "Original Sentence:\n",
      "They're all laughing at us, so we'll kick your ass.\n",
      "Reconstructed Sentence:\n",
      "they're all [MASK] at [MASK], so [MASK]'ll kick your [MASK].\n",
      "Original Sentence:\n",
      "Maine was very short on black people back then.\n",
      "Reconstructed Sentence:\n",
      "[MASK] was very [MASK] on black people [MASK] [MASK].\n",
      "Original Sentence:\n",
      "Briggs, what the hell's happening?\n",
      "Reconstructed Sentence:\n",
      "briggs, what the [MASK]'[MASK] happening?\n",
      "Original Sentence:\n",
      "Another one simply had no clue what to do, so whenever he met my brother he'd beat the crap out of him, and then say:\n",
      "Reconstructed Sentence:\n",
      "[MASK] [MASK] simply had no clue what [MASK] [MASK], so whenever [MASK] met [MASK] brother [MASK]'[MASK] [MASK] the [MASK] out of him, and [MASK] [MASK] :\n",
      "Original Sentence:\n",
      "I suppose you want me to buy you flowers and chocolates and whisper sweet nothings.\n",
      "Reconstructed Sentence:\n",
      "i suppose you [MASK] me [MASK] [MASK] you flowers and chocolates and whisper sweet [MASK]s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Extract and process the first 5 sentences\n",
    "sentences = df['reference'].tolist()\n",
    "\n",
    "# Define a threshold for toxic word prediction (adjust as needed)\n",
    "toxic_threshold = 0.7\n",
    "temp = 0\n",
    "\n",
    "# Define a set of punctuation marks\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Process and replace toxic words\n",
    "for sentence in tqdm(sentences):\n",
    "    encoded_dict = tokenizer(sentence)\n",
    "    tokens = encoded_dict['input_ids']\n",
    "    masked_sentence = []\n",
    "\n",
    "    # Initialize a flag to exclude special tokens\n",
    "    exclude_special_tokens = True\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token is a special token (CLS or SEP)\n",
    "        is_special_token = token in (tokenizer.cls_token_id, tokenizer.sep_token_id)\n",
    "\n",
    "        if exclude_special_tokens and is_special_token:\n",
    "            continue  # Skip special tokens\n",
    "        else:\n",
    "            # Convert the token to an index using your vocabulary mapping\n",
    "            inputs = torch.tensor([token], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predicted_prob = outputs.item()\n",
    "                # Check if the token is a punctuation mark\n",
    "                is_punctuation = tokenizer.convert_ids_to_tokens(token) in punctuation_set\n",
    "                if predicted_prob > toxic_threshold and not is_punctuation:\n",
    "                    masked_sentence.append(103) # mask token\n",
    "                else:\n",
    "                    masked_sentence.append(token)\n",
    "\n",
    "    # Use tokenizer.decode to reconstruct the sentence\n",
    "    reconstructed_sentence = tokenizer.decode(masked_sentence)\n",
    "\n",
    "    # Print the original sentence and the reconstructed sentence\n",
    "    print(\"Original Sentence:\")\n",
    "    print(sentence)\n",
    "    print(\"Reconstructed Sentence:\")\n",
    "    print(reconstructed_sentence)\n",
    "\n",
    "    temp += 1\n",
    "    if temp > 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_non_toxic_synonym(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        valid_synonyms = [synonym.lemmas()[0].name() for synonym in synonyms if synonym.lemmas()[0].name() != word and synonym.lemmas()[0].name() != '[UNK]']\n",
    "        if valid_synonyms:\n",
    "            return valid_synonyms[0]\n",
    "    return word  # If no valid synonyms found, return the original word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577777/577777 [04:10<00:00, 2309.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Extract and process the first 5 sentences\n",
    "sentences = df['reference'].tolist()\n",
    "recon = []\n",
    "\n",
    "# Define a threshold for toxic word prediction (adjust as needed)\n",
    "toxic_threshold = 0.7\n",
    "temp = 0\n",
    "\n",
    "# Define a set of punctuation marks\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Process and replace toxic words with synonyms\n",
    "for sentence in tqdm(sentences):\n",
    "    encoded_dict = tokenizer(sentence)\n",
    "    tokens = encoded_dict['input_ids']\n",
    "    replaced_sentence = []\n",
    "\n",
    "    # Initialize a flag to exclude special tokens\n",
    "    exclude_special_tokens = True\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token is a special token (CLS or SEP)\n",
    "        is_special_token = token in (tokenizer.cls_token_id, tokenizer.sep_token_id)\n",
    "\n",
    "        if exclude_special_tokens and is_special_token:\n",
    "            continue  # Skip special tokens\n",
    "        else:\n",
    "            # Convert the token to an index using your vocabulary mapping\n",
    "            inputs = torch.tensor([token], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predicted_prob = outputs.item()\n",
    "                # Check if the token is a punctuation mark\n",
    "                is_punctuation = tokenizer.convert_ids_to_tokens(token) in punctuation_set\n",
    "                if predicted_prob > toxic_threshold and not is_punctuation:\n",
    "                    # Replace toxic word with a non-toxic synonym\n",
    "                    word = tokenizer.convert_ids_to_tokens(token)\n",
    "                    non_toxic_synonym = get_non_toxic_synonym(word)\n",
    "                    if non_toxic_synonym in tokenizer.vocab:\n",
    "                        replaced_sentence.append(tokenizer.convert_tokens_to_ids(non_toxic_synonym))\n",
    "                    else:\n",
    "                        replaced_sentence.append(token)\n",
    "                else:\n",
    "                    replaced_sentence.append(token)\n",
    "\n",
    "    # Use tokenizer.decode to reconstruct the sentence\n",
    "    reconstructed_sentence = tokenizer.decode(replaced_sentence)\n",
    "\n",
    "    # Print the original sentence and the reconstructed sentenc\n",
    "    recon.append((sentence, reconstructed_sentence))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: If Alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.\n",
      "Non-toxic sentence: if alkar is flooding her with psychic waste, that explain the high level of neonurotransmitters.\n",
      "Original sentence: Now you're getting nasty.\n",
      "Non-toxic sentence: now you're acquiring nasty.\n",
      "Original sentence: Well, we could spare your life, for one.\n",
      "Non-toxic sentence: well, we could spare your life, for one.\n",
      "Original sentence: Ah! Monkey, you've got to snap out of it.\n",
      "Non-toxic sentence: ah! imp, you've got to snap out of it.\n",
      "Original sentence: I've got orders to put her down.\n",
      "Non-toxic sentence: i've got orders to put her down.\n",
      "Original sentence: I'm not gonna have a child... ...with the same genetic disorder as me who's gonna die. L...\n",
      "Non-toxic sentence: i'meter not gonna have a child...... with the same genetic disorder as me who'second gonna die. liter...\n",
      "Original sentence: They're all laughing at us, so we'll kick your ass.\n",
      "Non-toxic sentence: they're all laugh at us, so we'll kick your ass.\n",
      "Original sentence: Maine was very short on black people back then.\n",
      "Non-toxic sentence: maine was very short on black people rear then.\n",
      "Original sentence: Briggs, what the hell's happening?\n",
      "Non-toxic sentence: briggs, what the hell'second happening?\n",
      "Original sentence: Another one simply had no clue what to do, so whenever he met my brother he'd beat the crap out of him, and then say:\n",
      "Non-toxic sentence: another one simply had no clue what to bash, so whenever helium met my brother helium'd pulse the bullshit out of him, and then state :\n",
      "Original sentence: I suppose you want me to buy you flowers and chocolates and whisper sweet nothings.\n",
      "Non-toxic sentence: i suppose you want me to bargain you flowers and chocolates and whisper sweet nothings.\n"
     ]
    }
   ],
   "source": [
    "for i, rec in enumerate(recon):\n",
    "    print(\"Original sentence:\", rec[0])\n",
    "    print(\"Non-toxic sentence:\", rec[1])\n",
    "    if i == 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity of the original and detox sentences: 0.821828220463988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract the pairs of sentences from the tuples\n",
    "original = [sentence[0] for sentence in recon[:10000]]\n",
    "detox = [sentence[1] for sentence in recon[:10000]]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Combine the sentences for each array\n",
    "original_sentences = [' '.join(sentence.split()) for sentence in original]\n",
    "detox_sentences = [' '.join(sentence.split()) for sentence in detox]\n",
    "\n",
    "# Fit and transform the sentences to TF-IDF vectors\n",
    "original_tfidf = tfidf_vectorizer.fit_transform(original_sentences)\n",
    "detox_tfidf = tfidf_vectorizer.transform(detox_sentences)\n",
    "\n",
    "# Calculate cosine similarity for the corresponding sentences\n",
    "cosine_similarities = cosine_similarity(original_tfidf, detox_tfidf)\n",
    "\n",
    "cosine_similarity_sum = 0\n",
    "for i, sim in enumerate(cosine_similarities):\n",
    "    cosine_similarity_sum += sim[i]\n",
    "\n",
    "print(\"Mean cosine similarity of the original and detox sentences:\", cosine_similarity_sum / len(cosine_similarities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
