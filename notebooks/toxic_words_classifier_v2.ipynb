{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ToxicWordClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, dropout_rate=0.2):\n",
    "        super(ToxicWordClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)  # Apply dropout to the embedded input\n",
    "        output = self.fc(embedded)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load toxic and non-toxic words from external text files\n",
    "toxic_words = [line.strip() for line in open('/Users/damirabdulaev/Downloads/toxic_words.txt', 'r', encoding='utf-8')]\n",
    "non_toxic_words = [line.strip() for line in open('/Users/damirabdulaev/Downloads/positive-words.txt', 'r', encoding='utf-8')]\n",
    "all_words = toxic_words + non_toxic_words\n",
    "labels = [1] * len(toxic_words) + [0] * len(non_toxic_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and convert your sentences to model input\n",
    "sentences = all_words  # Replace with your list of sentences\n",
    "\n",
    "# Tokenize and convert sentences to input indices\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence and add special tokens\n",
    "    encoded_dict = tokenizer(\n",
    "        sentence,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=1,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Extract the input IDs and attention mask\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "# Convert the lists of tensors to a single tensor\n",
    "word_indices = torch.cat(input_ids, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1018],\n",
      "        [ 1019],\n",
      "        [ 1019],\n",
      "        ...,\n",
      "        [28672],\n",
      "        [27838],\n",
      "        [14101]])\n"
     ]
    }
   ],
   "source": [
    "print(word_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/7240 [00:00<?, ?it/s]/var/folders/8d/4djwtz9j45b6chzl1l6yzrg40000gn/T/ipykernel_8312/2386035275.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(indices, dtype=torch.long)\n",
      "Epoch 1:   1%|          | 83/7240 [00:01<01:43, 69.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, label)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Backpropagation and optimization\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     43\u001B[0m predicted \u001B[38;5;241m=\u001B[39m (outputs \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a PyTorch model\n",
    "vocab_size = len(tokenizer.vocab)  # Assuming you've defined 'vocabulary'\n",
    "embedding_dim = 100  # Adjust as needed\n",
    "output_dim = 1  # Assuming binary classification\n",
    "\n",
    "model = ToxicWordClassifier(vocab_size, embedding_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert labels to tensors\n",
    "labels = torch.tensor(labels, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Specify the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    # Wrap your training data with tqdm for the progress bar\n",
    "    for indices, label in tqdm(zip(word_indices, labels), total=len(labels), desc=f'Epoch {epoch + 1}'):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)[0][0]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct = (predicted == label).float()\n",
    "        total_correct += correct.sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = (total_correct / total_samples) * 100.0\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'twc.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "ToxicWordClassifier(\n  (embedding): Embedding(30522, 100)\n  (fc): Linear(in_features=100, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ToxicWordClassifier(vocab_size, embedding_dim, output_dim)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('twc.pth'))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                          reference   \n0           0  If Alkar is flooding her with psychic waste, t...  \\\n1           1                          Now you're getting nasty.   \n2           2           Well, we could spare your life, for one.   \n3           3          Ah! Monkey, you've got to snap out of it.   \n4           4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff   \n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309  \\\n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = '/Users/damirabdulaev/Downloads/filtered.tsv'\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "    Unnamed: 0                                          reference   \n22          22     Real life starts the first time you fuck, kid.  \\\n29          29                 Hey, leave the poor bastard alone!   \n41          41   It told you this was a waste of my fucking time.   \n43          43  I swear to God, the best thing I ever did in m...   \n48          48                  The guy is totally irresponsible.   \n\n                                          translation  similarity   \n22                    boy, real life starts up first.    0.866697  \\\n29                          leave the poor man alone!    0.857554   \n41            I told you this was a waste of my time.    0.904062   \n43  I swear to God, the best thing I've ever done ...    0.932305   \n48                     he's completely irresponsible.    0.862266   \n\n    lenght_diff   ref_tox   trn_tox  \n22     0.319149  0.998222  0.000114  \n29     0.257143  0.999382  0.000578  \n41     0.183673  0.995877  0.000479  \n43     0.022472  0.999071  0.000900  \n48     0.088235  0.643680  0.019941  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>Real life starts the first time you fuck, kid.</td>\n      <td>boy, real life starts up first.</td>\n      <td>0.866697</td>\n      <td>0.319149</td>\n      <td>0.998222</td>\n      <td>0.000114</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>Hey, leave the poor bastard alone!</td>\n      <td>leave the poor man alone!</td>\n      <td>0.857554</td>\n      <td>0.257143</td>\n      <td>0.999382</td>\n      <td>0.000578</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>It told you this was a waste of my fucking time.</td>\n      <td>I told you this was a waste of my time.</td>\n      <td>0.904062</td>\n      <td>0.183673</td>\n      <td>0.995877</td>\n      <td>0.000479</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>I swear to God, the best thing I ever did in m...</td>\n      <td>I swear to God, the best thing I've ever done ...</td>\n      <td>0.932305</td>\n      <td>0.022472</td>\n      <td>0.999071</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>The guy is totally irresponsible.</td>\n      <td>he's completely irresponsible.</td>\n      <td>0.862266</td>\n      <td>0.088235</td>\n      <td>0.643680</td>\n      <td>0.019941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df['similarity'] > 0.8) & (df['ref_tox'] > df['trn_tox'])]\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/damirabdulaev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/108569 [00:00<00:55, 1971.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "Real life starts the first time you fuck, kid.\n",
      "Reconstructed Sentence:\n",
      "real life starts the first time you [MASK], kid.\n",
      "Original Sentence:\n",
      "Hey, leave the poor bastard alone!\n",
      "Reconstructed Sentence:\n",
      "hey, [MASK] the [MASK] [MASK] alone!\n",
      "Original Sentence:\n",
      "It told you this was a waste of my fucking time.\n",
      "Reconstructed Sentence:\n",
      "[MASK] told you this [MASK] a [MASK] [MASK] [MASK] [MASK] time.\n",
      "Original Sentence:\n",
      "I swear to God, the best thing I ever did in my life was save that little son of a bitch\n",
      "Reconstructed Sentence:\n",
      "i [MASK] to god, the best thing i ever [MASK] in [MASK] life [MASK] save that [MASK] [MASK] [MASK] a [MASK]\n",
      "Original Sentence:\n",
      "The guy is totally irresponsible.\n",
      "Reconstructed Sentence:\n",
      "the [MASK] is totally irrespon [MASK].\n",
      "Original Sentence:\n",
      "\"Do you want to die?\" he said.\n",
      "Reconstructed Sentence:\n",
      "\" [MASK] you [MASK] to [MASK]? \" [MASK] said.\n",
      "Original Sentence:\n",
      "Does anal...\n",
      "Reconstructed Sentence:\n",
      "does [MASK]...\n",
      "Original Sentence:\n",
      "Your girlfriends are dead.\n",
      "Reconstructed Sentence:\n",
      "[MASK] girlfriends are dead.\n",
      "Original Sentence:\n",
      "Mikael was braver, or more foolhardy!\n",
      "Reconstructed Sentence:\n",
      "[MASK] [MASK] [MASK] braver, [MASK] more fool [MASK]y!\n",
      "Original Sentence:\n",
      "What the hell is going on?\n",
      "Reconstructed Sentence:\n",
      "what the [MASK] is [MASK] on?\n",
      "Original Sentence:\n",
      "I don't have to do shit.\n",
      "Reconstructed Sentence:\n",
      "i [MASK]'[MASK] have to [MASK] [MASK].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Extract and process the first 5 sentences\n",
    "sentences = df['reference'].tolist()\n",
    "\n",
    "# Define a threshold for toxic word prediction (adjust as needed)\n",
    "toxic_threshold = 0.7\n",
    "temp = 0\n",
    "\n",
    "# Define a set of punctuation marks\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Process and replace toxic words\n",
    "for sentence in tqdm(sentences):\n",
    "    encoded_dict = tokenizer(sentence)\n",
    "    tokens = encoded_dict['input_ids']\n",
    "    masked_sentence = []\n",
    "\n",
    "    # Initialize a flag to exclude special tokens\n",
    "    exclude_special_tokens = True\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token is a special token (CLS or SEP)\n",
    "        is_special_token = token in (tokenizer.cls_token_id, tokenizer.sep_token_id)\n",
    "\n",
    "        if exclude_special_tokens and is_special_token:\n",
    "            continue  # Skip special tokens\n",
    "        else:\n",
    "            # Convert the token to an index using your vocabulary mapping\n",
    "            inputs = torch.tensor([token], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predicted_prob = outputs.item()\n",
    "                # Check if the token is a punctuation mark\n",
    "                is_punctuation = tokenizer.convert_ids_to_tokens(token) in punctuation_set\n",
    "                if predicted_prob > toxic_threshold and not is_punctuation:\n",
    "                    masked_sentence.append(103) # mask token\n",
    "                else:\n",
    "                    masked_sentence.append(token)\n",
    "\n",
    "    # Use tokenizer.decode to reconstruct the sentence\n",
    "    reconstructed_sentence = tokenizer.decode(masked_sentence)\n",
    "\n",
    "    # Print the original sentence and the reconstructed sentence\n",
    "    print(\"Original Sentence:\")\n",
    "    print(sentence)\n",
    "    print(\"Reconstructed Sentence:\")\n",
    "    print(reconstructed_sentence)\n",
    "\n",
    "    temp += 1\n",
    "    if temp > 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_non_toxic_synonym(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        valid_synonyms = [synonym.lemmas()[0].name() for synonym in synonyms if synonym.lemmas()[0].name() != word and synonym.lemmas()[0].name() != '[UNK]']\n",
    "        if valid_synonyms:\n",
    "            return valid_synonyms[0]\n",
    "    return word  # If no valid synonyms found, return the original word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108569/108569 [01:04<00:00, 1684.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Extract and process the first 5 sentences\n",
    "sentences = df['reference'].tolist()\n",
    "recon = []\n",
    "\n",
    "# Define a threshold for toxic word prediction (adjust as needed)\n",
    "toxic_threshold = 0.7\n",
    "temp = 0\n",
    "\n",
    "# Define a set of punctuation marks\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Process and replace toxic words with synonyms\n",
    "for sentence in tqdm(sentences):\n",
    "    encoded_dict = tokenizer(sentence)\n",
    "    tokens = encoded_dict['input_ids']\n",
    "    replaced_sentence = []\n",
    "\n",
    "    # Initialize a flag to exclude special tokens\n",
    "    exclude_special_tokens = True\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token is a special token (CLS or SEP)\n",
    "        is_special_token = token in (tokenizer.cls_token_id, tokenizer.sep_token_id)\n",
    "\n",
    "        if exclude_special_tokens and is_special_token:\n",
    "            continue  # Skip special tokens\n",
    "        else:\n",
    "            # Convert the token to an index using your vocabulary mapping\n",
    "            inputs = torch.tensor([token], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predicted_prob = outputs.item()\n",
    "                # Check if the token is a punctuation mark\n",
    "                is_punctuation = tokenizer.convert_ids_to_tokens(token) in punctuation_set\n",
    "                if predicted_prob > toxic_threshold and not is_punctuation:\n",
    "                    # Replace toxic word with a non-toxic synonym\n",
    "                    word = tokenizer.convert_ids_to_tokens(token)\n",
    "                    non_toxic_synonym = get_non_toxic_synonym(word)\n",
    "                    if non_toxic_synonym in tokenizer.vocab:\n",
    "                        for element in tokenizer(non_toxic_synonym)['input_ids'][1:-1]:\n",
    "                            replaced_sentence.append(element)\n",
    "                    else:\n",
    "                        replaced_sentence.append(token)\n",
    "                else:\n",
    "                    replaced_sentence.append(token)\n",
    "\n",
    "    # Use tokenizer.decode to reconstruct the sentence\n",
    "    reconstructed_sentence = tokenizer.decode(replaced_sentence)\n",
    "\n",
    "    # Print the original sentence and the reconstructed sentenc\n",
    "    recon.append((sentence, reconstructed_sentence))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Specify the filename for saving the list\n",
    "file_name = 'recon.pkl'\n",
    "\n",
    "# Save the list to a file using pickle\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(recon, file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Specify the filename from which to load the list\n",
    "file_name = 'recon.pkl'\n",
    "\n",
    "recon = []\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_name, 'rb') as file:\n",
    "    recon = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Real life starts the first time you fuck, kid.\n",
      "Non-toxic sentence: real life starts the first time you fuck, kid.\n",
      "Original sentence: Hey, leave the poor bastard alone!\n",
      "Non-toxic sentence: hey, farewell the poor asshole alone!\n",
      "Original sentence: It told you this was a waste of my fucking time.\n",
      "Non-toxic sentence: it told you this was a waste of my fuck time.\n",
      "Original sentence: I swear to God, the best thing I ever did in my life was save that little son of a bitch\n",
      "Non-toxic sentence: i curse to god, the best thing i ever make in my life was save that small son of a bitch\n",
      "Original sentence: The guy is totally irresponsible.\n",
      "Non-toxic sentence: the guy is totally irrespon # # sible.\n",
      "Original sentence: \"Do you want to die?\" he said.\n",
      "Non-toxic sentence: \" bash you want to fail? \" helium said.\n",
      "Original sentence: Does anal...\n",
      "Non-toxic sentence: does anal...\n",
      "Original sentence: Your girlfriends are dead.\n",
      "Non-toxic sentence: your girlfriends are dead.\n",
      "Original sentence: Mikael was braver, or more foolhardy!\n",
      "Non-toxic sentence: mika # # el was braver, or more fool # # hardy!\n",
      "Original sentence: What the hell is going on?\n",
      "Non-toxic sentence: what the hell is departure on?\n",
      "Original sentence: I don't have to do shit.\n",
      "Non-toxic sentence: i don't have to bash crap.\n"
     ]
    }
   ],
   "source": [
    "for i, rec in enumerate(recon):\n",
    "    print(\"Original sentence:\", rec[0])\n",
    "    print(\"Non-toxic sentence:\", rec[1])\n",
    "    if i == 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/damirabdulaev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/damirabdulaev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity of the original and detox sentences: 0.8296287564356902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract the pairs of sentences from the tuples\n",
    "original = [sentence[0] for sentence in recon[:1000]]\n",
    "detox = [sentence[1] for sentence in recon[:1000]]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Combine the sentences for each array\n",
    "original_sentences = [' '.join(sentence.split()) for sentence in original]\n",
    "detox_sentences = [' '.join(sentence.split()) for sentence in detox]\n",
    "\n",
    "# Fit and transform the sentences to TF-IDF vectors\n",
    "original_tfidf = tfidf_vectorizer.fit_transform(original_sentences)\n",
    "detox_tfidf = tfidf_vectorizer.transform(detox_sentences)\n",
    "\n",
    "# Calculate cosine similarity for the corresponding sentences\n",
    "cosine_similarities = cosine_similarity(original_tfidf, detox_tfidf)\n",
    "\n",
    "cosine_similarity_sum = 0\n",
    "for i, sim in enumerate(cosine_similarities):\n",
    "    cosine_similarity_sum += sim[i]\n",
    "\n",
    "print(\"Mean cosine similarity of the original and detox sentences:\", cosine_similarity_sum / len(cosine_similarities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34436827139987874\n",
      "Calculate the semantic similarity\n",
      "0.8177248357832432\n",
      "Calculating CoLA acceptability stats\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23439ff8bc89481e89d11f9d03cb61bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271976281367242\n",
      "| ACC | SIM | FL | J |\n",
      "\n",
      "|0.3444|0.8177|0.9272|0.2611|\n",
      "\n",
      "0.2610974503268504\n"
     ]
    }
   ],
   "source": [
    "from J_metric import J\n",
    "\n",
    "result = J(original, detox)  # Call the J function with appropriate arguments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
